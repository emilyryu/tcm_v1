{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------------------\n",
    "# tcm_v1.py\n",
    "# Author: Emily Ryu\n",
    "# test for correlation between trades in two datsaets (qty and directionality).\n",
    "# v1 of the tcm (trade correlation metric) considers all trades within a window of\n",
    "#       size DELTA sec following trades in the first dataset.\n",
    "#-----------------------------------------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats.stats import pearsonr\n",
    "from scipy.stats import ttest_ind\n",
    "import random\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpickling\n",
    "filenames = glob.glob('azhang_intern_data/*.pkl')\n",
    "list_of_dfs = [pd.read_pickle(f) for f in filenames]\n",
    "\n",
    "# assign filename to each dataframe\n",
    "for dataframe, filename in zip(list_of_dfs, filenames):\n",
    "    start = filename.find('data/') + 5\n",
    "    end = filename.find('.pkl', start)\n",
    "    dataframe['filename'] = filename[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# create dictionary mapping filename to index\n",
    "files = []\n",
    "indices = []\n",
    "for x in range(len(list_of_dfs)):\n",
    "    files.append(list_of_dfs[x].iloc[0].filename)\n",
    "    indices.append(x)\n",
    "fileDict = dict(zip(files, indices))\n",
    "\n",
    "# sortedFileDict = dict(sorted(fileDict.items()))\n",
    "# for k, v in sortedFileDict.items(): print(k, v)\n",
    "    \n",
    "# for any date (YYYYMMDD), return a and b instrument indices as tuple (a,b)\n",
    "# works for every date starting at 07.21 except 2019.08.15 bc a has aA and aB\n",
    "def abIndices(date):\n",
    "    for key in fileDict.keys():\n",
    "        if date in key:\n",
    "            if ('aA' in key) or ('aB' in key):\n",
    "                aIndex = fileDict[key]\n",
    "            elif ('bA' in key) or ('bB' in key) or ('bC' in key):\n",
    "                bIndex = fileDict[key]\n",
    "    return (aIndex, bIndex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate correlation coefficient between trades in two dfs (volume and directionality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_trade_corr(df1, df2, deltasec):\n",
    "    # extract all trades from df1 and df2\n",
    "    # we just ignore NoSide trades bc there aren't very many of them, instead of determining based on prev bundle-\n",
    "    df1trades = df1.loc[(df1.msgtype == 'Trade') & ((df1.side == 'Sell') | (df1.side == 'Buy'))][['exchange_time', 'msgtype', 'qty', 'side']]\n",
    "    df2trades = df2.loc[(df2.msgtype == 'Trade') & ((df2.side == 'Sell') | (df2.side == 'Buy'))][['exchange_time', 'msgtype', 'qty', 'side']]\n",
    "    \n",
    "    # assign - sign to sellside trades\n",
    "    df1trades['sgn'] = [-1 if x == 'Sell' else 1 for x in df1trades['side']]\n",
    "    df1trades['sgn_qty'] = df1trades.qty * df1trades.sgn\n",
    "    df2trades['sgn'] = [-1 if x == 'Sell' else 1 for x in df2trades['side']]\n",
    "    df2trades['sgn_qty'] = df2trades.qty * df2trades.sgn\n",
    "    \n",
    "    # combine trades that occur at the exact same timestamp\n",
    "    # pandas groupby: info here https://realpython.com/pandas-groupby/, https://www.shanelynn.ie/summarising-aggregation-and-grouping-data-in-python-pandas/\n",
    "    # sum both qty and sgn_qty so we can look at total volume as well as directionality\n",
    "    times1 = df1trades.groupby('exchange_time').agg({'qty':sum, 'sgn_qty':sum})\n",
    "    times2 = df2trades.groupby('exchange_time').agg({'qty':sum, 'sgn_qty':sum})\n",
    "    \n",
    "    # for each timestamp X in times1, look at X to X+delta in df2. combine trades using the same code. plot 2 vs 1 (total magnitude and signed)\n",
    "    qties = []\n",
    "    sgn_qties = []\n",
    "    for time in times1.index:\n",
    "        timeUpperBound = time + datetime.timedelta(seconds=deltasec)\n",
    "        times2range = times2.loc[(times2.index >= time) & (times2.index <= timeUpperBound)]\n",
    "        qties.append((times1.loc[time].get('qty'), times2range.sum(0).get('qty')))\n",
    "        sgn_qties.append((times1.loc[time].get('sgn_qty'), times2range.sum(0).get('sgn_qty')))\n",
    "    \n",
    "    # calculate correlation\n",
    "    x_qt = [x[0] for x in qties]\n",
    "    y_qt = [x[1] for x in qties]\n",
    "    x_sqt = [x[0] for x in sgn_qties]\n",
    "    y_sqt = [x[1] for x in sgn_qties]\n",
    "    \n",
    "    # if y_qt or y_sqt is all zero, return 0 for that part of the correlation\n",
    "    if all(v == 0 for v in y_qt):\n",
    "        if all(w == 0 for w in y_sqt):\n",
    "            return (0,0)\n",
    "        return (0, pearsonr(x_sqt, y_sqt)[0])\n",
    "    if all(w == 0 for w in y_sqt):\n",
    "        return (pearsonr(x_qt, y_qt)[0], 0)\n",
    "    return (pearsonr(x_qt, y_qt)[0], pearsonr(x_sqt, y_sqt)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## correlation metrics -- for a single day, or for any two dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation metric for a single day, scaled by the two dfs compared with themselves\n",
    "def day_correlation(date, delta):   # note: date should be a string YYYYMMDD, delta is an int or float (seconds)\n",
    "    indices = abIndices(date)\n",
    "    dfA = list_of_dfs[indices[0]]\n",
    "    dfB = list_of_dfs[indices[1]]\n",
    "    unscaled = raw_trade_corr(dfA, dfB, delta)\n",
    "    scaleFactor0 = max(raw_trade_corr(dfA, dfA, delta)[0], raw_trade_corr(dfB, dfB, DELTA)[0])\n",
    "    scaleFactor1 = max(raw_trade_corr(dfA, dfA, delta)[1], raw_trade_corr(dfB, dfB, DELTA)[1])\n",
    "    return (unscaled[0] / scaleFactor0, unscaled[1] / scaleFactor1)\n",
    "\n",
    "# correlation metric testing for two dataframes, scaled by the two dfs compared with themselves\n",
    "def df_correlation(df1, df2, delta):   # note: date should be a string YYYYMMDD, delta is an int or float (seconds)\n",
    "    unscaled = raw_trade_corr(df1, df2, delta)\n",
    "    scaleFactor0 = max(abs(raw_trade_corr(df1, df1, delta)[0]), abs(raw_trade_corr(df2, df2, delta)[0]))\n",
    "    scaleFactor1 = max(abs(raw_trade_corr(df1, df1, delta)[1]), abs(raw_trade_corr(df2, df2, delta)[1]))\n",
    "    return (unscaled[0] / scaleFactor0, unscaled[1] / scaleFactor1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7557760649096326, 0.48934897798600546)\n",
      "(1.0, 1.0)\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "DELTA = 150\n",
    "df = list_of_dfs[abIndices('20191003')[1]]\n",
    "df.loc[df.msgtype == 'Trade'][['exchange_time', 'msgtype', 'qty', 'side']]\n",
    "print(day_correlation('20190722', DELTA))\n",
    "print(df_correlation(df, df, DELTA))  # should always be (1,1) when compared with self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a and b are related! (much more than random datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run day_correlation for every set of data we have\n",
    "# extract unique dates where we have data for both a and b\n",
    "dates = []\n",
    "for filename in fileDict.keys():\n",
    "    if 'bA' in filename or 'bB' in filename or 'bC' in filename:\n",
    "        if '20190815' not in filename:\n",
    "            dates.append(int(filename[3:]))      # represent as ints so that we can sort chronologically (something funky happens w strings)\n",
    "dates = sorted(dates)\n",
    "\n",
    "sameDateQties = []\n",
    "sameDateSqties = []\n",
    "for dateint in dates:\n",
    "    dc = day_correlation(str(dateint), DELTA)\n",
    "    sameDateQties.append(dc[0])\n",
    "    sameDateSqties.append(dc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7670740659165678, 0.7557760649096326, 0.663849684662856, 0.8235482440387778, 0.9941677932365808, 0.9083578366320779, 0.5670468078855363, 1.005478532161837, 0.36916779231287367, 0.7451819767713675, 0.806745967698566, 0.7028201570192756, 0.756575030421825, 0.6126218261336156, 0.6412991520129032, 0.4013556212922955, 0.3005753810268873, 0.6912114813783885, 0.7995960450895354, 0.5088647387957074, 0.7789022560344839, 0.7904716975159921, 0.3406030315218723, 0.8830131368730827, 0.7862998621962356, 0.5352607017376039, 0.6529747543155631, 0.24615194443258454, 0.5959062661934346, 0.6440287720696483, 0.5396547364361199, 0.2529679359040096, 0.34296253775858687, 0.19874140270648422, 0.5352594323333716, 0.3777389821662374, 0.6035272801296587, 0.3978761065729971, 0.2802806706400257, 0.42259487944069923, 0.3709970967983309, 0.28799024483267716, 0.2002569322964414, 0.6040323261291987, 0.41862223237324364, 0.3406380293537845, 0.567771381198451, 0.5851362396193788, 0.5004413340743956, 0.3875889901012069, 0.35657938850426424, 0.5498237562812193, 0.5714710993445472, 0.5725034481202702, 0.6258193460763112, 0.5909724108541444, 0.5055026933814414, 0.5455634637029289, 0.18653005922373514, 0.10659229057674545, -0.14778297079794067]\n",
      "[0.25667418133966596, 0.48934897798600546, 0.09888248429228773, 0.31778779335569696, 0.49565568888554207, -0.017501835791002358, -0.037493840187985496, 0.49245933772267636, 0.1936713967739955, 0.29625605990855425, 0.032296866156949496, 0.07732265822552041, 0.24157769159497472, 0.2420343050928125, 0.5164462560490903, 0.11688711194068649, 0.06874057274014074, 0.07804561865734926, 0.31142348258674923, 0.23058959618110558, 0.3009999048454188, 0.27569702067197704, 0.24995772192626242, -0.11820716415208736, 0.3396958520235906, 0.26162103205347653, 0.24782279713968802, 0.15356954624970218, 0.1388076822453946, 0.05120912528040392, 0.3529360495933877, 0.26892542985982837, 0.04027129499116337, 0.2291922042444104, 0.24418892524911034, 0.22801093350444643, 0.1843258735004999, 0.2644000743182775, 0.026102195170369352, 0.14056901130512034, 0.3211160014080111, -0.009457134922992222, 0.24359886708590328, 0.17007173621215024, 0.1818272494465888, 0.36988002221941435, -0.1672598308016657, 0.22120775399090442, 0.07820103620253806, 0.18955606554955132, 0.0886884807424866, 0.22456912061764087, 0.12042265858287855, 0.06370054120802529, 0.13834264384012693, 0.051428847286626306, 0.22384865920314845, 0.04201084983216964, 0.23497130242064104, 0.0188911761492763, 0.16289964532676898]\n",
      "0.5369439406298454\n",
      "0.18606091155953197\n"
     ]
    }
   ],
   "source": [
    "print(sameDateQties)\n",
    "print(sameDateSqties)\n",
    "print(np.mean(sameDateQties))\n",
    "print(np.mean(sameDateSqties))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run df_correlation for a set of pairs of random dfs\n",
    "randomTuples = []\n",
    "randomQties = []\n",
    "randomSqties = []\n",
    "\n",
    "for i in range(len(dates)):\n",
    "    index1 = random.randrange(len(list_of_dfs))\n",
    "    index2 = random.randrange(len(list_of_dfs))\n",
    "    while (index2 == index1):\n",
    "        index2 = random.randrange(len(list_of_dfs)) # make sure we're not comparing with self\n",
    "    randomTuples.append((index1,index2))\n",
    "    dfc = df_correlation(list_of_dfs[index1], list_of_dfs[index2], DELTA)\n",
    "    randomQties.append(dfc[0])\n",
    "    randomSqties.append(dfc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(43, 124), (36, 110), (50, 112), (39, 31), (54, 36), (94, 3), (29, 68), (125, 14), (51, 68), (75, 106), (74, 34), (19, 42), (20, 68), (108, 14), (55, 109), (107, 54), (23, 27), (48, 126), (17, 68), (101, 62), (109, 107), (96, 107), (128, 21), (38, 101), (23, 103), (62, 17), (71, 9), (28, 78), (20, 87), (62, 16), (123, 10), (117, 44), (52, 67), (100, 86), (121, 113), (32, 104), (83, 19), (55, 95), (52, 120), (83, 15), (18, 43), (39, 46), (3, 97), (67, 127), (99, 78), (68, 49), (89, 115), (93, 66), (110, 119), (12, 83), (0, 112), (42, 47), (24, 20), (0, 47), (87, 14), (103, 119), (109, 75), (104, 99), (38, 59), (1, 83), (57, 40)]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "print(randomTuples)\n",
    "print(randomQties)\n",
    "print(randomSqties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttest_indResult(statistic=18.45340340265046, pvalue=2.044640621503051e-26)\n",
      "Ttest_indResult(statistic=10.197043615930488, pvalue=1.015910848302365e-14)\n"
     ]
    }
   ],
   "source": [
    "# t test for statistcal significance difference bt the two\n",
    "print(ttest_ind(sameDateQties, randomQties, equal_var = False))\n",
    "print(ttest_ind(sameDateSqties, randomSqties, equal_var = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a leading b has greater correlation than b leading a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def day_correlation_BA(date, delta):   # note: date should be a string YYYYMMDD, delta is an int (seconds)\n",
    "    indices = abIndices(date)\n",
    "    dfA = list_of_dfs[indices[0]]\n",
    "    dfB = list_of_dfs[indices[1]]\n",
    "    unscaled = raw_trade_corr(dfB, dfA, delta)   # switch order of b and a code otherwise same\n",
    "    scaleFactor0 = max(raw_trade_corr(dfA, dfA, delta)[0], raw_trade_corr(dfB, dfB, DELTA)[0])\n",
    "    scaleFactor1 = max(raw_trade_corr(dfA, dfA, delta)[1], raw_trade_corr(dfB, dfB, DELTA)[1])\n",
    "    return (unscaled[0] / scaleFactor0, unscaled[1] / scaleFactor1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5837122980400278, 0.5453005387361219, 0.2887315282030423, 0.6241472313424755, 0.4956321080084221, 0.2646199454942585, -0.17913191624685548, 0.25421261765007575, 0.1949917512265915, 0.6324796694208624, 0.7131090319646339, 0.8082460804592848, 0.7310364186952805, 0.15222065483099112, 0.3265363892071796, 0.39373895210148113, 0.493417350900276, 0.33873150990085366, 0.5439157451975081, 0.4711865958466211, 0.1823467145835507, 0.3354516512828031, 0.2821043393579923, 0.6939543948251553, 0.3229953262942533, 0.46664944761790755, 0.2523545086531893, 0.10406619811173887, 0.8895898305493074, 0.6346422094339784, 0.5476863513662183, 0.7584993325551227, 0.8134915717113833, 0.1239928451516413, 0.48451695013504525, 0.4571501338256851, 0.08652357833477002, 0.5756421593995734, 0.596702599098158, 0.22597811578881719, 0.19820786254712622, 0.4268810941865628, 0.04705425638449214, 0.4462576882129789, 0.16110888388522474, 0.41016831679011456, 1.0039421929835664, 0.44885753626194785, 0.6786378323255333, 0.6185132528547113, 0.6024616954777207, 0.2902361362638252, 0.6532384194676932, 0.23346659656522606, 0.5574423462400838, 0.8576569572805439, 0.4272879745651107, 0.33693798935122454, 0.37458056806459605, 0.11738500822289441, -0.033243035935630744]\n",
      "[0.34655618236176927, 0.4224310209605947, 0.39163085360490746, 0.5087227634598297, 0.42572720001899006, -0.03925731842790284, 0.1964497193951524, 0.14561405808772152, 0.21769938450575416, 0.17035869650222327, 0.18004700146548955, -0.05530250213184425, 0.0361721471595826, 0.05853221042716213, 0.018166829194707464, 0.24848099854016206, 0.3816159194190693, 0.005621583802844964, 0.5253575464470891, 0.128588337953681, 0.23883075913749952, 0.11063151581598522, 0.1646055909826937, -0.04868457282710152, 0.2501699344130342, 0.5847967538474148, 0.31206063867822065, -0.010316659504766781, 0.7016008692548844, 0.24240580138750115, 0.47391312336900143, 0.1876489669265365, -0.06870262935111972, 0.045811286722037804, -0.03667533460620796, 0.27318720122381485, 0.21784238225363572, 0.2901586069329588, 0.09568770908031335, 0.17907628120796507, 0.41714768628990945, 0.2893652272742045, 0.17780620260932622, 0.4887943898310526, 0.06579608346192066, 0.05296574310873457, 0.1537344783980375, 0.19733392174146588, -0.005896816167407483, 0.1414839839703149, 0.06437102579462028, 0.23957249658933502, 0.23979111847861342, 0.01053493156699726, 0.18348798605084676, 0.3310416053157341, 0.19782557860052605, 0.20835204583465802, 0.20804818265555014, 0.2289475139681357, 0.11026629942615403]\n"
     ]
    }
   ],
   "source": [
    "sameDateQtiesBA = []\n",
    "sameDateSqtiesBA = []\n",
    "for dateint in dates:\n",
    "    dc = day_correlation_BA(str(dateint), DELTA)\n",
    "    sameDateQtiesBA.append(dc[0])\n",
    "    sameDateSqtiesBA.append(dc[1])\n",
    "print(sameDateQtiesBA)\n",
    "print(sameDateSqtiesBA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43226643165657325\n",
      "0.20521361545065595\n",
      "Ttest_indResult(statistic=2.4549019929931633, pvalue=0.015533142510015007)\n",
      "Ttest_indResult(statistic=-0.6737064389185259, pvalue=0.5018336757513682)\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(sameDateQtiesBA))\n",
    "print(np.mean(sameDateSqtiesBA))\n",
    "print(ttest_ind(sameDateQties, sameDateQtiesBA, equal_var = False))\n",
    "print(ttest_ind(sameDateSqties, sameDateSqtiesBA, equal_var = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
